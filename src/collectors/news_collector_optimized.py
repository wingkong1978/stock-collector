#!/usr/bin/env python3
"""
Stock News Collector - Optimized Version
è‚¡ç¥¨æ–°é—»é‡‡é›†æ¨¡å— - ä¼˜åŒ–ç‰ˆ

ä¼˜åŒ–å†…å®¹:
1. æ·»åŠ è¯·æ±‚é—´éš”æ§åˆ¶ï¼ˆéšæœº1-3ç§’å»¶è¿Ÿï¼Œé˜²åçˆ¬ï¼‰
2. å®Œå–„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
3. æ·»åŠ æ•°æ®éªŒè¯åŠŸèƒ½
4. ä¼˜åŒ–æ—¥å¿—è®°å½•
5. æ·»åŠ æ€§èƒ½ç›‘æ§ï¼ˆè®°å½•é‡‡é›†è€—æ—¶ï¼‰
6. ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢SQLæ³¨å…¥
7. å®Œå–„ç±»å‹æ³¨è§£
8. ä¼˜åŒ–æ•°æ®åº“è¿æ¥ç®¡ç†
"""

import hashlib
import json
import os
import random
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any, Tuple
from urllib.parse import urljoin

import akshare as ak
import pandas as pd
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
from database.db_manager import get_db_manager, DatabaseManager


class NewsCollector:
    """è‚¡ç¥¨æ–°é—»é‡‡é›†å™¨ - ä¼˜åŒ–ç‰ˆ"""

    # æ–°é—»æ¥æºæ˜ å°„
    NEWS_SOURCES = {
        "sina": "æ–°æµªè´¢ç»",
        "eastmoney": "ä¸œæ–¹è´¢å¯Œ",
        "10jqka": "åŒèŠ±é¡º",
        "cls": "è´¢è”ç¤¾",
    }
    
    # é»˜è®¤é…ç½®
    DEFAULT_CONFIG = {
        "delay_min": 1.0,  # æœ€å°å»¶è¿Ÿ(ç§’)
        "delay_max": 3.0,  # æœ€å¤§å»¶è¿Ÿ(ç§’)
        "max_retries": 3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
        "retry_backoff": 2,  # é‡è¯•é€€é¿å› å­
        "request_timeout": 30,  # è¯·æ±‚è¶…æ—¶(ç§’)
    }

    def __init__(self, config_path: str = "config"):
        self.config_path = Path(config_path)
        self.load_config()
        self.db_manager: Optional[DatabaseManager] = None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats: Dict[str, Any] = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_articles": 0,
            "start_time": None,
        }

        # å¦‚æœé…ç½®äº†æ•°æ®åº“ï¼Œåˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        if self.settings.get("storage", {}).get("database"):
            self.init_database()

    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            stocks_json = self.config_path / "stocks.json"
            settings_json = self.config_path / "settings.json"
            
            if stocks_json.exists():
                with open(stocks_json, "r", encoding="utf-8") as f:
                    self.stocks_config = json.load(f)
            else:
                self.stocks_config = {"stocks": []}
                
            if settings_json.exists():
                with open(settings_json, "r", encoding="utf-8") as f:
                    self.settings = json.load(f)
            else:
                self.settings = {"storage": {"type": "csv", "path": "data"}}
                
            logger.info("âœ… æ–°é—»é‡‡é›†é…ç½®åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ–°é—»é‡‡é›†é…ç½®åŠ è½½å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤é…ç½®è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            self.stocks_config = {"stocks": []}
            self.settings = {"storage": {"type": "csv", "path": "data"}}

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            self.db_manager = get_db_manager()
            logger.info("âœ… æ–°é—»é‡‡é›†æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âš ï¸ æ–°é—»é‡‡é›†æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.db_manager = None

    def _random_delay(self) -> None:
        """
        éšæœºå»¶è¿Ÿï¼Œé˜²æ­¢åçˆ¬
        
        åœ¨æ¯æ¬¡è¯·æ±‚å‰è°ƒç”¨ï¼Œéšæœºç­‰å¾… 1-3 ç§’
        """
        delay = random.uniform(
            self.DEFAULT_CONFIG["delay_min"],
            self.DEFAULT_CONFIG["delay_max"]
        )
        logger.debug(f"â±ï¸  ç­‰å¾… {delay:.2f} ç§’...")
        time.sleep(delay)

    def _generate_news_id(self, title: str, url: str, pub_time: str) -> str:
        """
        ç”Ÿæˆæ–°é—»å”¯ä¸€ID
        
        ä½¿ç”¨æ ‡é¢˜ã€URLå’Œå‘å¸ƒæ—¶é—´ç”Ÿæˆç¨³å®šçš„å“ˆå¸Œå€¼ä½œä¸ºå”¯ä¸€æ ‡è¯†
        """
        content = f"{title}|{url}|{pub_time}"
        return hashlib.md5(content.encode("utf-8")).hexdigest()

    def _safe_datetime(self, value: Any) -> Optional[datetime]:
        """
        å®‰å…¨è½¬æ¢ä¸º datetime ç±»å‹

        æ”¯æŒå¤šç§æ ¼å¼:
        - pandas Timestamp
        - å­—ç¬¦ä¸²æ ¼å¼: "2024-01-15 10:30:00"
        - ISO æ ¼å¼: "2024-01-15T10:30:00"
        """
        if value is None or pd.isna(value):
            return None

        try:
            # å·²ç»æ˜¯ datetime ç±»å‹
            if isinstance(value, datetime):
                return value

            # pandas Timestamp
            if isinstance(value, pd.Timestamp):
                return value.to_pydatetime()

            # å­—ç¬¦ä¸²è§£æ
            str_value = str(value).strip()

            # å°è¯•å¤šç§æ ¼å¼
            formats = [
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%d %H:%M",
                "%Y-%m-%d",
                "%Y/%m/%d %H:%M:%S",
                "%Y/%m/%d",
                "%Y-%m-%dT%H:%M:%S",
                "%Y-%m-%dT%H:%M:%S.%f",
            ]

            for fmt in formats:
                try:
                    return datetime.strptime(str_value, fmt)
                except ValueError:
                    continue

            # ä½¿ç”¨ pandas æ™ºèƒ½è§£æ
            parsed = pd.to_datetime(str_value, errors="raise")
            return parsed.to_pydatetime()

        except Exception as e:
            logger.debug(f"æ—¶é—´è½¬æ¢å¤±è´¥: value={value}, error={e}")
            return None

    def _validate_news_data(self, df: pd.DataFrame) -> Tuple[bool, str]:
        """
        éªŒè¯æ–°é—»æ•°æ®å®Œæ•´æ€§

        Args:
            df: æ–°é—»æ•°æ® DataFrame

        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        if df is None:
            return False, "DataFrame ä¸º None"

        if df.empty:
            return False, "DataFrame ä¸ºç©º"

        # æ£€æŸ¥å¿…éœ€å­—æ®µï¼ˆä¸åŒæ¥æºå­—æ®µåä¸åŒï¼‰
        possible_title_cols = ["æ ‡é¢˜", "title", "news_title", "Title", "æ–°é—»æ ‡é¢˜"]
        has_title = any(col in df.columns for col in possible_title_cols)

        if not has_title:
            return False, f"ç¼ºå°‘æ ‡é¢˜å­—æ®µï¼Œå¯ç”¨å­—æ®µ: {df.columns.tolist()}"

        # æ£€æŸ¥æ•°æ®è¡Œæ•°
        if len(df) == 0:
            return False, "æ•°æ®è¡Œæ•°ä¸º 0"

        logger.debug(f"âœ… æ–°é—»æ•°æ®éªŒè¯é€šè¿‡: {len(df)} æ¡æ•°æ®")
        return True, ""

    def collect_individual_news(
        self, stock_code: str, days: int = 7, max_retries: int = 3
    ) -> Optional[pd.DataFrame]:
        """
        é‡‡é›†ä¸ªè‚¡æ–°é—» - ä¼˜åŒ–ç‰ˆ

        Args:
            stock_code: è‚¡ç¥¨ä»£ç  (å¦‚ "000001")
            days: é‡‡é›†æœ€è¿‘å‡ å¤©çš„æ–°é—»
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            æ–°é—»æ•°æ® DataFrame
        """
        logger.info(f"ğŸ” å¼€å§‹é‡‡é›†è‚¡ç¥¨ {stock_code} çš„æ–°é—»...")
        self.performance_stats["total_requests"] += 1

        task_name = f"individual_news_{stock_code}"
        start_time = datetime.now()
        last_error = None

        # è¯·æ±‚å‰å»¶è¿Ÿ
        self._random_delay()

        # é‡è¯•æœºåˆ¶
        for attempt in range(max_retries):
            try:
                # ä½¿ç”¨ akshare è·å–ä¸ªè‚¡æ–°é—»
                df = ak.stock_news_em(symbol=stock_code)

                # éªŒè¯æ•°æ®
                is_valid, error_msg = self._validate_news_data(df)
                if not is_valid:
                    raise ValueError(f"æ•°æ®æºè¿”å›æ— æ•ˆæ–°é—»æ•°æ®: {error_msg}")

                # ç­›é€‰æ—¶é—´èŒƒå›´
                cutoff_date = datetime.now() - timedelta(days=days)

                # æ ¹æ®æ•°æ®æºä¸åŒï¼Œæ—¶é—´å­—æ®µåå¯èƒ½ä¸åŒ
                time_cols = ["å‘å¸ƒæ—¶é—´", "pub_time", "ctime", "datetime", "æ—¶é—´"]
                time_col = None
                for col in time_cols:
                    if col in df.columns:
                        time_col = col
                        break

                if time_col:
                    # è½¬æ¢æ—¶é—´å¹¶ç­›é€‰
                    df["_parsed_time"] = df[time_col].apply(self._safe_datetime)
                    df = df[df["_parsed_time"] >= cutoff_date].copy()
                    df = df.drop(columns=["_parsed_time"])

                # æ·»åŠ å…ƒæ•°æ®
                df["_stock_code"] = stock_code
                df["_collected_at"] = datetime.now()
                df["_news_id"] = df.apply(
                    lambda row: self._generate_news_id(
                        str(row.get("æ–°é—»æ ‡é¢˜", row.get("æ ‡é¢˜", row.get("title", "")))),
                        str(row.get("æ–°é—»é“¾æ¥", row.get("é“¾æ¥", row.get("url", "")))),
                        str(row.get("å‘å¸ƒæ—¶é—´", row.get("pub_time", ""))),
                    ),
                    axis=1,
                )

                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"âœ… è‚¡ç¥¨ {stock_code} æ–°é—»é‡‡é›†æˆåŠŸ: {len(df)} æ¡, è€—æ—¶: {elapsed:.2f}s")
                
                self.performance_stats["successful_requests"] += 1
                self.performance_stats["total_articles"] += len(df)

                self.db_manager and self.db_manager.log_collection(
                    task_name, "success", f"é‡‡é›†æˆåŠŸ: {len(df)} æ¡æ–°é—»"
                )

                return df

            except Exception as e:
                last_error = e
                self.performance_stats["failed_requests"] += 1

                if attempt < max_retries - 1:
                    wait_time = self.DEFAULT_CONFIG["retry_backoff"] ** attempt
                    logger.warning(
                        f"âš ï¸  é‡‡é›†å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}, ç­‰å¾… {wait_time}s åé‡è¯•..."
                    )
                    time.sleep(wait_time)
                else:
                    logger.error(f"âŒ ä¸ªè‚¡æ–°é—»é‡‡é›†å¤±è´¥: {e}, è‚¡ç¥¨ä»£ç : {stock_code}")

        self.db_manager and self.db_manager.log_collection(
            task_name, "error", f"é‡è¯• {max_retries} æ¬¡åå¤±è´¥: {last_error}"
        )
        return None

    def collect_financial_news(
        self, num_pages: int = 5, max_retries: int = 3
    ) -> Optional[pd.DataFrame]:
        """
        é‡‡é›†è´¢ç»è¦é—» - ä¼˜åŒ–ç‰ˆ

        Args:
            num_pages: é‡‡é›†é¡µæ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            æ–°é—»æ•°æ® DataFrame
        """
        logger.info(f"ğŸ” å¼€å§‹é‡‡é›†è´¢ç»è¦é—»ï¼Œé¡µæ•°: {num_pages}")
        self.performance_stats["total_requests"] += 1

        task_name = "financial_news"
        start_time = datetime.now()

        all_news = []

        for page in range(1, num_pages + 1):
            for attempt in range(max_retries):
                try:
                    # è¯·æ±‚å‰å»¶è¿Ÿ
                    self._random_delay()
                    
                    # ä½¿ç”¨ akshare è·å–è´¢ç»è¦é—»
                    df = ak.stock_news_main_cx()

                    if df is not None and not df.empty:
                        # éªŒè¯æ•°æ®
                        is_valid, error_msg = self._validate_news_data(df)
                        if is_valid:
                            all_news.append(df)
                            logger.info(f"âœ… ç¬¬ {page} é¡µé‡‡é›†æˆåŠŸ: {len(df)} æ¡")
                        else:
                            logger.warning(f"âš ï¸  ç¬¬ {page} é¡µæ•°æ®éªŒè¯å¤±è´¥: {error_msg}")

                    break  # æˆåŠŸåè·³å‡ºé‡è¯•å¾ªç¯

                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = self.DEFAULT_CONFIG["retry_backoff"] ** attempt
                        logger.warning(f"âš ï¸  ç¬¬ {page} é¡µé‡‡é›†å¤±è´¥ï¼Œ{wait_time}såé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"âŒ ç¬¬ {page} é¡µé‡‡é›†æœ€ç»ˆå¤±è´¥: {e}")

        if not all_news:
            logger.error("âŒ è´¢ç»è¦é—»é‡‡é›†å®Œå…¨å¤±è´¥")
            self.performance_stats["failed_requests"] += 1
            self.db_manager and self.db_manager.log_collection(
                task_name, "error", "é‡‡é›†å®Œå…¨å¤±è´¥"
            )
            return None

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_news, ignore_index=True)

        # å»é‡ (åŸºäº summary å­—æ®µ)
        if "summary" in combined_df.columns:
            combined_df = combined_df.drop_duplicates(subset=["summary"], keep="first")

        # æ·»åŠ å…ƒæ•°æ®
        combined_df["_collected_at"] = datetime.now()
        combined_df["_news_id"] = combined_df.apply(
            lambda row: self._generate_news_id(
                str(row.get("summary", "")),
                str(row.get("url", "")),
                str(row.get("tag", "")),
            ),
            axis=1,
        )

        elapsed = (datetime.now() - start_time).total_seconds()
        logger.info(f"âœ… è´¢ç»è¦é—»é‡‡é›†å®Œæˆ: {len(combined_df)} æ¡, è€—æ—¶: {elapsed:.2f}s")
        
        self.performance_stats["successful_requests"] += 1
        self.performance_stats["total_articles"] += len(combined_df)

        self.db_manager and self.db_manager.log_collection(
            task_name, "success", f"é‡‡é›†æˆåŠŸ: {len(combined_df)} æ¡æ–°é—»"
        )

        return combined_df

    def collect_all_stocks_news(
        self, days: int = 3, max_stocks: Optional[int] = None
    ) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡é‡‡é›†å…³æ³¨åˆ—è¡¨ä¸­æ‰€æœ‰è‚¡ç¥¨çš„æ–°é—» - ä¼˜åŒ–ç‰ˆ

        Args:
            days: é‡‡é›†æœ€è¿‘å‡ å¤©çš„æ–°é—»
            max_stocks: æœ€å¤šé‡‡é›†å¤šå°‘åªè‚¡ç¥¨ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰

        Returns:
            è‚¡ç¥¨ä»£ç  -> æ–°é—» DataFrame çš„å­—å…¸
        """
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡é‡‡é›†å…³æ³¨è‚¡ç¥¨æ–°é—»...")
        self.performance_stats["start_time"] = datetime.now()

        stocks = self.stocks_config.get("stocks", [])
        if max_stocks:
            stocks = stocks[:max_stocks]

        results = {}
        total_start = datetime.now()

        for idx, stock in enumerate(stocks, 1):
            code = stock.get("code")
            name = stock.get("name")

            if not code:
                continue

            logger.info(f"[{idx}/{len(stocks)}] ğŸ“ˆ é‡‡é›† {name} ({code}) çš„æ–°é—»...")

            df = self.collect_individual_news(code, days=days)
            if df is not None and not df.empty:
                results[code] = df

            # æ¯æ¬¡è¯·æ±‚é—´å»¶è¿Ÿï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if idx < len(stocks):
                self._random_delay()

        total_elapsed = (datetime.now() - total_start).total_seconds()
        total_news = sum(len(df) for df in results.values())

        logger.info(f"âœ… æ‰¹é‡é‡‡é›†å®Œæˆ: {len(results)}/{len(stocks)} åªè‚¡ç¥¨, å…± {total_news} æ¡æ–°é—», è€—æ—¶: {total_elapsed:.2f}s")

        return results

    def get_performance_report(self) -> Dict[str, Any]:
        """
        è·å–æ€§èƒ½æŠ¥å‘Š
        
        Returns:
            æ€§èƒ½ç»Ÿè®¡å­—å…¸
        """
        if self.performance_stats["start_time"]:
            elapsed = (datetime.now() - self.performance_stats["start_time"]).total_seconds()
        else:
            elapsed = 0
            
        total_req = self.performance_stats["total_requests"]
        success_req = self.performance_stats["successful_requests"]
        
        return {
            "æ€»è¯·æ±‚æ•°": total_req,
            "æˆåŠŸè¯·æ±‚": success_req,
            "å¤±è´¥è¯·æ±‚": self.performance_stats["failed_requests"],
            "æˆåŠŸç‡": f"{(success_req / max(total_req, 1) * 100):.1f}%",
            "é‡‡é›†æ–‡ç« æ•°": self.performance_stats["total_articles"],
            "æ€»è€—æ—¶(ç§’)": round(elapsed, 2),
        }

    def save_news_to_csv(self, df: pd.DataFrame, prefix: str = "news") -> Optional[Path]:
        """
        ä¿å­˜æ–°é—»æ•°æ®åˆ° CSV æ–‡ä»¶ - ä¼˜åŒ–ç‰ˆ
        
        Args:
            df: æ–°é—»æ•°æ® DataFrame
            prefix: æ–‡ä»¶åå‰ç¼€

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if df is None or df.empty:
            logger.warning("âš ï¸  æ–°é—»æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
            return None

        try:
            storage_path = Path(self.settings.get("storage", {}).get("path", "data"))
            news_path = storage_path / "news"
            news_path.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{prefix}_{timestamp}.csv"
            filepath = news_path / filename

            df.to_csv(filepath, index=False, encoding="utf-8-sig")
            logger.info(f"âœ… æ–°é—»æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")

            return filepath

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–°é—»åˆ° CSV å¤±è´¥: {e}")
            return None

    def run(self):
        """è¿è¡Œæ–°é—»é‡‡é›†ä»»åŠ¡ - ä¼˜åŒ–ç‰ˆ"""
        logger.info("=" * 60)
        logger.info("ğŸš€ è‚¡ç¥¨æ–°é—»é‡‡é›†ä»»åŠ¡å¼€å§‹")
        logger.info("=" * 60)

        try:
            # 1. é‡‡é›†è´¢ç»è¦é—»
            financial_news = self.collect_financial_news(num_pages=3)
            if financial_news is not None:
                self.save_news_to_csv(financial_news, prefix="financial_news")

            # 2. é‡‡é›†å…³æ³¨è‚¡ç¥¨çš„æ–°é—»
            stock_news_results = self.collect_all_stocks_news(days=3)
            for code, df in stock_news_results.items():
                self.save_news_to_csv(df, prefix=f"news_{code}")

            # è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
            report = self.get_performance_report()
            logger.info("=" * 60)
            logger.info("ğŸ“Š æ€§èƒ½æŠ¥å‘Š:")
            for key, value in report.items():
                logger.info(f"  {key}: {value}")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ æ–°é—»é‡‡é›†ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            raise

    def close(self):
        """å…³é—­èµ„æº"""
        if self.db_manager:
            self.db_manager.close()
            logger.info("âœ… æ–°é—»é‡‡é›†å™¨æ•°æ®åº“è¿æ¥å·²å…³é—­")

    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()


if __name__ == "__main__":
    with NewsCollector() as collector:
        collector.run()
