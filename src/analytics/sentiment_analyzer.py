#!/usr/bin/env python3
"""
News Sentiment Analyzer
æ–°é—»æƒ…æ„Ÿåˆ†ææ¨¡å—
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import re
from datetime import datetime
from typing import Dict, List, Optional
from collections import Counter
from loguru import logger
import json


class SentimentAnalyzer:
    """æ–°é—»æƒ…æ„Ÿåˆ†æå™¨"""
    
    # æ­£é¢è¯æ±‡è¯å…¸
    POSITIVE_WORDS = [
        'ä¸Šæ¶¨', 'å¤§æ¶¨', 'æš´æ¶¨', 'é£™å‡', 'æ¶¨åœ', 'çªç ´', 'åˆ©å¥½', 'å¼ºåŠ²',
        'å¢é•¿', 'æå‡', 'æ”¹å–„', 'ä¼˜åŒ–', 'åˆ›æ–°', 'é¢†å…ˆ', 'ä¼˜åŠ¿', 'æˆåŠŸ',
        'ç›ˆåˆ©', 'å¢æ”¶', 'æ‰©å¼ ', 'åˆä½œ', 'è®¢å•', 'äº¤ä»˜', 'é‡äº§', 'çˆ¬å¡',
        'è¶…é¢„æœŸ', 'çœ‹å¥½', 'æ¨è', 'ä¹°å…¥', 'å¢æŒ', 'ç›®æ ‡ä»·', 'ä¸Šè°ƒ',
        'åå¼¹', 'å›æš–', 'å¤è‹', 'æ™¯æ°”', 'é«˜å¢', 'çˆ†å‘', 'æ‹ç‚¹'
    ]
    
    # è´Ÿé¢è¯æ±‡è¯å…¸
    NEGATIVE_WORDS = [
        'ä¸‹è·Œ', 'å¤§è·Œ', 'æš´è·Œ', 'è·Œåœ', 'å´©ç›˜', 'ç ´ä½', 'åˆ©ç©º', 'ç–²è½¯',
        'ä¸‹é™', 'ä¸‹æ»‘', 'äºæŸ', 'å‡å°‘', 'è£å‘˜', 'å…³é—­', 'é€€å‡º', 'å¤±è´¥',
        'æš´é›·', 'è¿çº¦', 'è¯‰è®¼', 'è°ƒæŸ¥', 'å¤„ç½š', 'é€€å¸‚', 'é£é™©', 'è­¦ç¤º',
        'ä¸‹è°ƒ', 'å–å‡º', 'å‡æŒ', 'çœ‹ç©º', 'å›é¿', ' downgrade',
        'æ”¾ç¼“', 'æ”¶ç¼©', 'ä½è¿·', 'å¯’å†¬', 'æ‰¿å‹', 'æ‹–ç´¯', 'ä¸åŠé¢„æœŸ'
    ]
    
    # ä¸­æ€§è¡Œä¸šè¯æ±‡ï¼ˆè¿‡æ»¤ç”¨ï¼‰
    NEUTRAL_WORDS = [
        'è‚¡ç¥¨', 'è‚¡å¸‚', 'è¯åˆ¸', 'å¸‚åœº', 'æ¿å—', 'è¡Œä¸š', 'æ¦‚å¿µ',
        'æ¶¨å¹…', 'è·Œå¹…', 'æˆäº¤é¢', 'æˆäº¤é‡', 'æ¢æ‰‹ç‡', 'å¸‚ç›ˆç‡',
        'ä¸»åŠ›èµ„é‡‘', 'å‡€æµå…¥', 'å‡€æµå‡º'
    ]
    
    def __init__(self, data_path: str = "data"):
        self.data_path = Path(data_path)
        self.news_path = self.data_path / "news"
        self.analytics_path = self.data_path / "analytics"
        self.analytics_path.mkdir(parents=True, exist_ok=True)
    
    def load_news_data(self, stock_code: Optional[str] = None, days: int = 30) -> pd.DataFrame:
        """åŠ è½½æ–°é—»æ•°æ®"""
        if stock_code:
            pattern = f"news_{stock_code}_*.csv"
        else:
            pattern = "news_*.csv"
        
        files = list(self.news_path.glob(pattern))
        
        if not files:
            logger.warning("æœªæ‰¾åˆ°æ–°é—»æ•°æ®æ–‡ä»¶")
            return pd.DataFrame()
        
        # æŒ‰æ—¶é—´æ’åº
        files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        all_data = []
        for file in files[:days * 2]:  # è¯»å–æ›´å¤šæ–‡ä»¶
            try:
                df = pd.read_csv(file)
                all_data.append(df)
            except Exception as e:
                logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file}: {e}")
                continue
        
        if not all_data:
            return pd.DataFrame()
        
        combined = pd.concat(all_data, ignore_index=True)
        
        # å»é‡
        if 'æ–°é—»æ ‡é¢˜' in combined.columns:
            combined = combined.drop_duplicates(subset=['æ–°é—»æ ‡é¢˜'], keep='first')
        
        return combined
    
    def analyze_sentiment(self, text: str) -> Dict:
        """åˆ†æå•æ¡æ–‡æœ¬çš„æƒ…æ„Ÿ"""
        if not text or pd.isna(text):
            return {'score': 0, 'label': 'neutral', 'confidence': 0}
        
        text = str(text)
        
        # ç»Ÿè®¡æ­£è´Ÿè¯æ±‡
        positive_count = sum(1 for word in self.POSITIVE_WORDS if word in text)
        negative_count = sum(1 for word in self.NEGATIVE_WORDS if word in text)
        
        # è®¡ç®—æƒ…æ„Ÿå¾—åˆ† (-1 åˆ° 1)
        total = positive_count + negative_count
        if total == 0:
            return {'score': 0, 'label': 'neutral', 'confidence': 0}
        
        score = (positive_count - negative_count) / total
        
        # ç¡®å®šæ ‡ç­¾
        if score > 0.2:
            label = 'positive'
        elif score < -0.2:
            label = 'negative'
        else:
            label = 'neutral'
        
        # ç½®ä¿¡åº¦
        confidence = min(total / 3, 1.0)
        
        return {
            'score': float(score),
            'label': label,
            'confidence': float(confidence),
            'positive_words': positive_count,
            'negative_words': negative_count
        }
    
    def analyze_news_sentiment(self, stock_code: Optional[str] = None) -> Dict:
        """åˆ†ææ–°é—»æƒ…æ„Ÿ"""
        logger.info(f"å¼€å§‹åˆ†ææ–°é—»æƒ…æ„Ÿ...")
        
        df = self.load_news_data(stock_code)
        
        if df.empty:
            logger.warning("æ²¡æœ‰æ–°é—»æ•°æ®å¯ä¾›åˆ†æ")
            return {}
        
        results = []
        
        # åˆ†ææ¯æ¡æ–°é—»
        for idx, row in df.iterrows():
            content = ""
            if 'æ–°é—»æ ‡é¢˜' in row and pd.notna(row['æ–°é—»æ ‡é¢˜']):
                content += str(row['æ–°é—»æ ‡é¢˜'])
            if 'æ–°é—»å†…å®¹' in row and pd.notna(row['æ–°é—»å†…å®¹']):
                content += " " + str(row['æ–°é—»å†…å®¹'])
            
            sentiment = self.analyze_sentiment(content)
            
            result = {
                'title': row.get('æ–°é—»æ ‡é¢˜', ''),
                'publish_time': row.get('å‘å¸ƒæ—¶é—´', ''),
                'source': row.get('æ–‡ç« æ¥æº', ''),
                'sentiment': sentiment['label'],
                'score': sentiment['score'],
                'confidence': sentiment['confidence']
            }
            results.append(result)
        
        # ç»Ÿè®¡
        sentiments = [r['sentiment'] for r in results]
        sentiment_counts = Counter(sentiments)
        
        total = len(results)
        positive_pct = sentiment_counts.get('positive', 0) / total * 100
        negative_pct = sentiment_counts.get('negative', 0) / total * 100
        neutral_pct = sentiment_counts.get('neutral', 0) / total * 100
        
        # è®¡ç®—å¹³å‡æƒ…æ„Ÿå¾—åˆ†
        avg_score = sum(r['score'] for r in results) / total if total > 0 else 0
        
        analysis = {
            'stock_code': stock_code,
            'analysis_time': datetime.now().isoformat(),
            'total_news': total,
            'sentiment_distribution': {
                'positive': {'count': sentiment_counts.get('positive', 0), 'percentage': round(positive_pct, 2)},
                'negative': {'count': sentiment_counts.get('negative', 0), 'percentage': round(negative_pct, 2)},
                'neutral': {'count': sentiment_counts.get('neutral', 0), 'percentage': round(neutral_pct, 2)}
            },
            'average_sentiment_score': round(avg_score, 4),
            'overall_sentiment': 'positive' if avg_score > 0.1 else ('negative' if avg_score < -0.1 else 'neutral'),
            'news_details': results[:20]  # åªä¿å­˜å‰20æ¡è¯¦æƒ…
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        self._save_sentiment_report(stock_code, analysis)
        
        return analysis
    
    def analyze_keywords(self, stock_code: Optional[str] = None, top_n: int = 20) -> Dict:
        """åˆ†æå…³é”®è¯"""
        df = self.load_news_data(stock_code)
        
        if df.empty:
            return {}
        
        # æå–æ‰€æœ‰æ–‡æœ¬
        all_text = ""
        for idx, row in df.iterrows():
            if 'æ–°é—»æ ‡é¢˜' in row and pd.notna(row['æ–°é—»æ ‡é¢˜']):
                all_text += str(row['æ–°é—»æ ‡é¢˜']) + " "
            if 'æ–°é—»å†…å®¹' in row and pd.notna(row['æ–°é—»å†…å®¹']):
                all_text += str(row['æ–°é—»å†…å®¹']) + " "
        
        # ç®€å•çš„åˆ†è¯ï¼ˆåŸºäºç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
        words = re.findall(r'[\u4e00-\u9fa5]{2,}', all_text)
        
        # è¿‡æ»¤ä¸­æ€§è¯å’Œåœç”¨è¯
        filtered_words = [w for w in words if w not in self.NEUTRAL_WORDS and len(w) >= 2]
        
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(filtered_words)
        
        return {
            'top_keywords': word_counts.most_common(top_n),
            'total_words': len(words),
            'unique_words': len(word_counts)
        }
    
    def _save_sentiment_report(self, stock_code: Optional[str], report: Dict):
        """ä¿å­˜æƒ…æ„Ÿåˆ†ææŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        code_suffix = f"_{stock_code}" if stock_code else ""
        filename = f"sentiment{code_suffix}_{timestamp}.json"
        filepath = self.analytics_path / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æƒ…æ„Ÿåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {filepath}")
    
    def generate_sentiment_summary(self, stock_code: str) -> str:
        """ç”Ÿæˆæƒ…æ„Ÿåˆ†ææ‘˜è¦æ–‡æœ¬"""
        analysis = self.analyze_news_sentiment(stock_code)
        keywords = self.analyze_keywords(stock_code, top_n=10)
        
        if not analysis:
            return "æš‚æ— æ–°é—»æ•°æ®"
        
        summary = f"""
ğŸ“Š {stock_code} æ–°é—»æƒ…æ„Ÿåˆ†æ

ğŸ“° æ–°é—»æ•°é‡: {analysis['total_news']} æ¡

ğŸ“ˆ æƒ…æ„Ÿåˆ†å¸ƒ:
   æ­£é¢: {analysis['sentiment_distribution']['positive']['count']} æ¡ ({analysis['sentiment_distribution']['positive']['percentage']}%)
   è´Ÿé¢: {analysis['sentiment_distribution']['negative']['count']} æ¡ ({analysis['sentiment_distribution']['negative']['percentage']}%)
   ä¸­æ€§: {analysis['sentiment_distribution']['neutral']['count']} æ¡ ({analysis['sentiment_distribution']['neutral']['percentage']}%)

ğŸ¯ æ•´ä½“æƒ…æ„Ÿ: {analysis['overall_sentiment']}
ğŸ“Š å¹³å‡å¾—åˆ†: {analysis['average_sentiment_score']:.4f}
"""
        
        if keywords.get('top_keywords'):
            summary += "\nğŸ”¥ çƒ­é—¨å…³é”®è¯:\n"
            for word, count in keywords['top_keywords'][:5]:
                summary += f"   - {word}: {count}æ¬¡\n"
        
        return summary


if __name__ == "__main__":
    # æµ‹è¯•
    analyzer = SentimentAnalyzer()
    
    # æƒ…æ„Ÿåˆ†æ
    sentiment = analyzer.analyze_news_sentiment("600584")
    print(json.dumps(sentiment, ensure_ascii=False, indent=2))
    
    # å…³é”®è¯åˆ†æ
    keywords = analyzer.analyze_keywords("600584")
    print("\nå…³é”®è¯:", keywords)
    
    # æ‘˜è¦
    summary = analyzer.generate_sentiment_summary("600584")
    print("\n" + summary)
